{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TMjMfLMkBm6x"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T02:16:50.309271Z",
     "start_time": "2019-10-31T02:16:40.826609Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HKl2zpiw3z3D"
   },
   "outputs": [],
   "source": [
    "# built-in utilities\n",
    "import copy\n",
    "import os\n",
    "from glob import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# data tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, explained_variance_score, mean_squared_log_error, mean_absolute_error, median_absolute_error, mean_squared_error, r2_score, confusion_matrix, roc_curve, accuracy_score, roc_auc_score, homogeneity_score, completeness_score, classification_report, silhouette_samples\n",
    "\n",
    "# pytorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "!pip install --upgrade grpcio>=1.24.3\n",
    "!pip install --upgrade google-auth~=1.4.0\n",
    "!pip install --upgrade tensorflow>=2.0.0\n",
    "!pip install tensorboard\n",
    "\n",
    "# colab\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3x1oZ3f4o2p"
   },
   "outputs": [],
   "source": [
    "# import project tools\n",
    "import dataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBa8w_Yj3z3Y"
   },
   "source": [
    "# Sort data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUQVGA-L3z3Z"
   },
   "source": [
    "## Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T21:08:28.312891Z",
     "start_time": "2019-10-02T20:39:39.366630Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Zca6AD0R3z3a"
   },
   "outputs": [],
   "source": [
    "# # sort files into folders\n",
    "# dataLoader.imageSort(\n",
    "#     destinationRoot=\"~/Downloads/TrainBinary\",\n",
    "#     imageRoot=\"~/Downloads/ISIC_2019_Training_Input\",\n",
    "#     labelDf=labelDf,\n",
    "#     sortType=\"binary\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T02:27:09.275469Z",
     "start_time": "2019-10-31T02:27:09.239148Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "gHwwRtI_3z3Q"
   },
   "outputs": [],
   "source": [
    "image_root = \"/content/drive/BinaryData/MEL\"\n",
    "files = glob(os.path.join(image_root, \"*.jpg\"))\n",
    "print(\"Total number of melanoma images: {}\\n\".format(len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcyFTM1K9Kro"
   },
   "outputs": [],
   "source": [
    "image_root = \"/content/drive/BinaryData/NONMEL\"\n",
    "files = glob(os.path.join(image_root, \"*.jpg\"))\n",
    "print(\"Total number of non-melanoma images: {}\\n\".format(len(files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUUpjOl13z3d"
   },
   "source": [
    "## Multi class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T19:41:00.687116Z",
     "start_time": "2019-10-02T19:18:38.977353Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "vGZizrC13z3e"
   },
   "outputs": [],
   "source": [
    "# # sort files into folders\n",
    "# dataLoader.imageSort(\n",
    "#     destinationRoot=\"~/Downloads/TrainMulti\",\n",
    "#     imageRoot=\"~/Downloads/ISIC_2019_Training_Input\",\n",
    "#     labelDf=labelDf,\n",
    "#     sortType=\"multi\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fyg5A9Hd3z3i"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0EKpDidfAEbn"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "data_path = \"/content/drive/BinaryData\"\n",
    "data = torchvision.datasets.ImageFolder(root=data_path, transform=transform)\n",
    "\n",
    "print(\"Classes: {}\".format(data.classes))\n",
    "print(\"Value counts: {}\".format(np.unique(np.array(data.targets), return_counts=True)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f3DR3yNyZMqa"
   },
   "source": [
    "## Visualize raw images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jSFVOIt9Re7Z"
   },
   "outputs": [],
   "source": [
    "def image_sample(inp, title=None, figsize=(20,20)):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(inp, interpolation='nearest')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "# create data loader to visualize image batch grid\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    data,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "inputs, classes = next(iter(loader))\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "image_sample(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1X53cxG3z3J"
   },
   "source": [
    "## Class labels\n",
    "\n",
    "- MEL: “Melanoma” diagnosis confidence\n",
    "- NV: “Melanocytic nevus” diagnosis confidence\n",
    "- BCC: “Basal cell carcinoma” diagnosis confidence\n",
    "- AK: “Actinic keratosis” diagnosis confidence\n",
    "- BKL: “Benign keratosis (solar lentigo / seborrheic keratosis / lichen planus-like keratosis)” diagnosis confidence\n",
    "- DF: “Dermatofibroma” diagnosis confidence\n",
    "- VASC: “Vascular lesion” diagnosis confidence\n",
    "- SCC: \"Squamous cell carcinoma\"\n",
    "- UNK: None of the others / \"out of distribution\" diagnosis confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T02:18:34.078790Z",
     "start_time": "2019-10-31T02:18:31.401485Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SCftpodP3z3V"
   },
   "outputs": [],
   "source": [
    "# load and prepare label dataframe\n",
    "label_df = dataLoader.processLabelDf(\n",
    "    file=\"ISIC_2019_Training_GroundTruth.csv\"\n",
    ")\n",
    "label_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFrLn9x6_2Uu"
   },
   "source": [
    "## Train/validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFJlPXECMkFD"
   },
   "source": [
    "### Stratified indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qk0iBxek-8vL"
   },
   "outputs": [],
   "source": [
    "# statify by target label\n",
    "train_ix, validation_ix = train_test_split(\n",
    "    np.arange(len(data.targets)),\n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    "    shuffle=True,\n",
    "    stratify=data.targets\n",
    ")\n",
    "\n",
    "print(np.unique(np.array(data.targets)[train_ix], return_counts=True))\n",
    "print(np.unique(np.array(data.targets)[validation_ix], return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCQiUH6FMq97"
   },
   "source": [
    "### Split original data into subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwilkVQRM3qr"
   },
   "outputs": [],
   "source": [
    "## use indexes to subset full dataset\n",
    "# train\n",
    "train_samples = [data.imgs[i][0] for i in train_ix]\n",
    "train_targets = [data.targets[i] for i in train_ix]\n",
    "train_targets = torch.LongTensor(train_targets)\n",
    "\n",
    "# validation\n",
    "validation_samples = [data.imgs[i][0] for i in validation_ix]\n",
    "validation_targets = [data.targets[i] for i in validation_ix]\n",
    "validation_targets = torch.LongTensor(validation_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CnLJEVnTMcMe"
   },
   "outputs": [],
   "source": [
    "class ISICDatasetTrain(Dataset):\n",
    "    def __init__(self, image_paths, targets, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index])\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# training transformations\n",
    "# normMean = [0.763038, 0.54564667, 0.57004464]\n",
    "# normStd = [0.14092727, 0.15261286, 0.1699712]\n",
    "\n",
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(norm_mean,\n",
    "        #                      norm_std)\n",
    "    ])\n",
    "\n",
    "train_data = ISICDatasetTrain(\n",
    "    image_paths=train_samples,\n",
    "    targets=train_targets,\n",
    "    # image_paths=train_samples[:250],\n",
    "    # targets=train_targets[:250],\n",
    "    transform=train_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5uwfXz1M6Gr"
   },
   "outputs": [],
   "source": [
    "class ISICDatasetTest(Dataset):\n",
    "    def __init__(self, image_paths, targets, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index])\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# validation transformations\n",
    "validation_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(size=224),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(norm_mean,\n",
    "    #                      norm_std)\n",
    "])\n",
    "\n",
    "validation_data = ISICDatasetTest(\n",
    "    image_paths=validation_samples,\n",
    "    targets=validation_targets,\n",
    "    # image_paths=validation_samples[:250],\n",
    "    # targets=validation_targets[:250],\n",
    "    transform=validation_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e6W-9NJvWZ_S"
   },
   "source": [
    "### Class weighting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WAf0m3X9NwoP"
   },
   "outputs": [],
   "source": [
    "# weights\n",
    "sample_count = np.array([len(np.where(train_data.targets==t)[0]) for t in np.unique(train_data.targets)])\n",
    "weight = 1. / sample_count\n",
    "indv_weights = np.array([weight[int(t)] for t in train_data.targets])\n",
    "indv_weights = torch.from_numpy(indv_weights)\n",
    "print(\"Class weights: {}\".format(indv_weights.unique()))\n",
    "\n",
    "# create sampler object to be used in data loader\n",
    "weighted_sampler = torch.utils.data.WeightedRandomSampler(indv_weights.type('torch.FloatTensor'), len(indv_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVnALpILPmW5"
   },
   "outputs": [],
   "source": [
    "# # Iterate DataLoader and check class balance for each batch\n",
    "# demo_loader = DataLoader(\n",
    "#     trainData, \n",
    "#     batch_size=16,\n",
    "#     num_workers=1,\n",
    "#     sampler=weighted_sampler\n",
    "# )\n",
    "\n",
    "# for i, (x, y) in enumerate(demo_loader):\n",
    "#     print(\"batch index {}, 0/1: {}/{}\".format(\n",
    "#         i, (y == 0).sum(), (y == 1).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YauD4Xfk-8vB"
   },
   "source": [
    "### Train/validation data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k5UwfKU0-8uy"
   },
   "outputs": [],
   "source": [
    "## create data loaders\n",
    "# train\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    sampler=weighted_sampler\n",
    ")\n",
    "\n",
    "# validation\n",
    "validation_data_loader = torch.utils.data.DataLoader(\n",
    "    validation_data,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yo_zbJA1YH0z"
   },
   "source": [
    "### Visualize transformed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DuVBYweYK-t"
   },
   "outputs": [],
   "source": [
    "# visualize image batch grid\n",
    "inputs, classes = next(iter(train_data_loader))\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "image_sample(\n",
    "    out,\n",
    "    # title=[data.classes[x] for x in classes]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfJFpX8SYe13"
   },
   "outputs": [],
   "source": [
    "# visualize image batch grid\n",
    "inputs, classes = next(iter(validation_data_loader))\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "image_sample(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqFxcRQqTxiJ"
   },
   "source": [
    "# References\n",
    "\n",
    "https://www.kaggle.com/xinruizhuang/skin-lesion-classification-acc-90-pytorch\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html\n",
    "\n",
    "https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard\n",
    "\n",
    "https://pytorch.org/docs/stable/tensorboard.html\n",
    "\n",
    "https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce\n",
    "\n",
    "https://towardsdatascience.com/https-medium-com-dinber19-take-a-deeper-look-at-your-pytorch-model-with-the-new-tensorboard-built-in-513969cf6a72\n",
    "\n",
    "https://github.com/andyhahaha/Uncertainty-Mnist-with-Pytorch\n",
    "\n",
    "https://discuss.pytorch.org/t/using-nn-dropout2d-at-eval-time-for-modelling-uncertainty/45274\n",
    "\n",
    "https://xuwd11.github.io/Dropout_Tutorial_in_PyTorch/\n",
    "\n",
    "https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4F2nEMh780aT"
   },
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YiUutK9DlEL"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZjbeHgzrkmUj"
   },
   "source": [
    "### Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B-C4KSv3TlOT"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        \n",
    "        self.fc1 = nn.Linear(32*56*56, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(-1, 32*56*56)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qYxQz8iTWWKw"
   },
   "source": [
    "### Pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZYXR0yed-cQN"
   },
   "source": [
    "#### VGG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j7-zlpP7-cdy"
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLT7qRHZ_H2D"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdFaVf6n_VIT"
   },
   "outputs": [],
   "source": [
    "print(model.classifier[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSskvTWx-_5o"
   },
   "outputs": [],
   "source": [
    "# freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# customize last step of model to the this task\n",
    "model.classifier[6] = nn.Sequential(\n",
    "    nn.Linear(4096, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(512, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Mo8o-N6ATpM"
   },
   "outputs": [],
   "source": [
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLJb8DCzAxKT"
   },
   "outputs": [],
   "source": [
    "# Find total parameters and trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"{:,} total parameters\".format(total_params))\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"{:,} trainable parameters\".format(total_trainable_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hg2iQ0Wt-XGh"
   },
   "source": [
    "#### resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EqNYv641WXyG"
   },
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(512, 10),\n",
    "                                 nn.LogSoftmax(dim=1))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxpT4-p7QmfR"
   },
   "source": [
    "## Training loop / validation set evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jf3gqrPGQmsP"
   },
   "outputs": [],
   "source": [
    "class PyTorchTrainer:\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        # random seed settings\n",
    "        self.seed = config.seed\n",
    "        torch.manual_seed(self.seed)\n",
    "        self.verbose = config.verbose\n",
    "\n",
    "        # data loaders\n",
    "        self.train_data_loader = config.train_data_loader\n",
    "        self.validation_data_loader = config.validation_data_loader\n",
    "\n",
    "        ## model object creation and device assignment\n",
    "        self.device = config.device\n",
    "\n",
    "        # if passing in the name of model Class object\n",
    "        if isinstance(config.model, type):\n",
    "            self.model = config.model().to(self.device)\n",
    "        # if model is already instantiated, or if transfer learning model is used\n",
    "        else:\n",
    "            self.model = config.model.to(self.device)\n",
    "\n",
    "        # name to use when saving model state\n",
    "        if config.model_name is not None:\n",
    "            self.model_name = config.model_name\n",
    "        else:\n",
    "            self.model_name = \"untitled\"\n",
    "\n",
    "        # model training settings\n",
    "        self.lr = config.lr\n",
    "        self.epochs = config.epochs\n",
    "        self.optimizer = config.optimizer(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = config.criterion\n",
    "\n",
    "        self.n_epochs_stop = 5\n",
    "        self.min_val_loss = np.inf\n",
    "        self.epochs_no_improve = 0\n",
    "\n",
    "        ## load previous state\n",
    "        # use checkpoint to load model state and associated objects\n",
    "        if config.model_object_dir is not None:\n",
    "            print(\">>> Resuming training...\")\n",
    "            self.model_object_dir = config.model_object_dir\n",
    "\n",
    "            # establish directory\n",
    "            self.model_dir = os.path.join(self.model_object_dir, \"models\")\n",
    "            self.object_dir = os.path.join(self.model_object_dir, \"objects\")\n",
    "            self.log_dir = os.path.join(self.model_object_dir, \"logs\")\n",
    "            self.log_train_dir = os.path.join(self.model_object_dir, \"logs\",\"train\")\n",
    "            self.log_validation_dir = os.path.join(self.model_object_dir, \"logs\",\"validation\")\n",
    "            \n",
    "            # load model\n",
    "            self.model.load_state_dict(torch.load(os.path.join(self.model_dir, os.listdir(self.model_dir)[0])))\n",
    "            self.model = self.model.to(self.device)\n",
    "            self.model_name = os.listdir(self.model_dir)[0].split(\".\")[0]\n",
    "            \n",
    "            # load statistics objects\n",
    "            self.running_avg_train_f1 = torch.load(os.path.join(self.object_dir, \"running_avg_train_f1.pt\"))\n",
    "            self.running_avg_train_precision = torch.load(os.path.join(self.object_dir, \"running_avg_train_precision.pt\"))\n",
    "            self.running_avg_train_recall = torch.load(os.path.join(self.object_dir, \"running_avg_train_recall.pt\"))\n",
    "            self.running_avg_train_accuracy = torch.load(os.path.join(self.object_dir, \"running_avg_train_accuracy.pt\"))\n",
    "            self.running_avg_train_loss = torch.load(os.path.join(self.object_dir, \"running_avg_train_loss.pt\"))\n",
    "            \n",
    "            self.running_avg_validation_f1 = torch.load(os.path.join(self.object_dir, \"running_avg_validation_f1.pt\"))\n",
    "            self.running_avg_validation_precision = torch.load(os.path.join(self.object_dir, \"running_avg_validation_precision.pt\"))\n",
    "            self.running_avg_validation_recall = torch.load(os.path.join(self.object_dir, \"running_avg_validation_recall.pt\"))\n",
    "            self.running_avg_validation_accuracy = torch.load(os.path.join(self.object_dir, \"running_avg_validation_accuracy.pt\"))\n",
    "            self.running_avg_validation_loss = torch.load(os.path.join(self.object_dir, \"running_avg_validation_loss.pt\"))\n",
    "\n",
    "            self.globaliter = torch.load(os.path.join(self.object_dir, \"globaliter.pt\"))\n",
    "    \n",
    "        else:\n",
    "            # directory tree for storing model attributes\n",
    "            current = datetime.today().strftime('%Y%m%d_%H%M') + \"_\" + self.model_name\n",
    "            \n",
    "            self.model_object_dir = os.path.join(os.getcwd(), \"model_objects\", current)\n",
    "            self.model_dir = os.path.join(self.model_object_dir, \"models\")\n",
    "            self.object_dir = os.path.join(self.model_object_dir, \"objects\")\n",
    "            self.log_dir = os.path.join(self.model_object_dir, \"logs\")\n",
    "            self.log_train_dir = os.path.join(self.model_object_dir, \"logs\",\"train\")\n",
    "            self.log_validation_dir = os.path.join(self.model_object_dir, \"logs\",\"validation\")\n",
    "            \n",
    "            os.makedirs(self.model_object_dir, exist_ok=True)\n",
    "            os.makedirs(self.model_dir, exist_ok=True)\n",
    "            os.makedirs(self.object_dir, exist_ok=True)\n",
    "            os.makedirs(self.log_dir, exist_ok=True)\n",
    "            os.makedirs(self.log_train_dir, exist_ok=True)\n",
    "            os.makedirs(self.log_validation_dir, exist_ok=True)\n",
    "            \n",
    "            self.globaliter = 0\n",
    "\n",
    "        # tensorboard\n",
    "        self.tensorboard_files = config.tensorboard_files\n",
    "        if self.tensorboard_files:\n",
    "            self.train_summary_writer = SummaryWriter(self.log_train_dir)\n",
    "            self.validation_summary_writer = SummaryWriter(self.log_validation_dir)\n",
    "        else:\n",
    "            self.train_summary_writer = None\n",
    "            self.validation_summary_writer = None\n",
    "            \n",
    "        self.beginning_time = time.time()\n",
    "\n",
    "    def train(self, epoch):\n",
    "        epoch_preds = []\n",
    "        epoch_targets = []\n",
    "\n",
    "        epoch_f1 = []\n",
    "        epoch_precision = []\n",
    "        epoch_recall = []\n",
    "        epoch_accuracy = []\n",
    "        epoch_loss = []\n",
    "\n",
    "        self.globaliter += 1\n",
    "        epoch_beginning_time = time.time()\n",
    "        \n",
    "        # sample batch number for data capture\n",
    "        num_batches = np.floor(len(self.train_data_loader.dataset.image_paths) / self.train_data_loader.batch_size)\n",
    "        sample_batch_idx = np.random.randint(0, num_batches)\n",
    "\n",
    "        self.model.train()\n",
    "        print(\"*\" * 100)\n",
    "        for batch_idx, (data, target) in enumerate(self.train_data_loader):\n",
    "            batch_beginning_time = time.time()\n",
    "\n",
    "            data = data.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "\n",
    "            output = self.model(data)\n",
    "            train_loss = self.criterion(output, target)\n",
    "            epoch_loss.append(train_loss.item())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            #Metrics\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            epoch_preds = epoch_preds + pred.detach().cpu().numpy().tolist()\n",
    "            epoch_targets = epoch_targets + target.detach().cpu().numpy().tolist()\n",
    "\n",
    "            metric_f1 = f1_score(epoch_targets, epoch_preds)\n",
    "            epoch_f1.append(metric_f1)\n",
    "\n",
    "            metric_precision = precision_score(epoch_targets, epoch_preds)\n",
    "            epoch_precision.append(metric_precision)\n",
    "\n",
    "            metric_recall = recall_score(epoch_targets, epoch_preds)\n",
    "            epoch_recall.append(metric_recall)\n",
    "\n",
    "            metric_accuracy = accuracy_score(epoch_targets, epoch_preds)\n",
    "            epoch_accuracy.append(metric_accuracy)\n",
    "\n",
    "            # print progress report\n",
    "            if self.verbose:\n",
    "                if batch_idx % 50 == 0 and batch_idx > 0:\n",
    "                    print(\"\\nTrain epoch: {} | Batch: {} | [Processed {}/{} ({:.0f}%)]\\n\\tLoss: {:.6f} | F1: {:.6f} | Precision: {:.6f} | Recall: {:.6f} | Accuracy: {:.6f}\".format(\n",
    "                        epoch, batch_idx, len(epoch_preds), len(self.train_data_loader.dataset),\n",
    "                        100. * len(epoch_preds) / len(self.train_data_loader.dataset), train_loss.item(), metric_f1,\n",
    "                        metric_precision, metric_recall, metric_accuracy))\n",
    "                    print(\"\\tBatch time elapsed: {}\\n\".format(self.train_timer(batch_beginning_time, time.time())))\n",
    "                    print(\"\\n\" + \"*\" * 10)\n",
    "\n",
    "            # # image batch sample\n",
    "            # if batch_idx == sample_batch_idx:\n",
    "            #     image_grid = torchvision.utils.make_grid(data.cpu())\n",
    "            #     self.train_summary_writer.add_image('train/Sample batch', image_grid, global_step=self.globaliter)\n",
    "            \n",
    "        # mark epoch end timestamp\n",
    "        epoch_ending_time = time.time()\n",
    "\n",
    "        try:\n",
    "            self.running_avg_train_f1.append((sum(epoch_f1) / len(epoch_f1)))\n",
    "            self.running_avg_train_precision.append((sum(epoch_precision) / len(epoch_precision)))\n",
    "            self.running_avg_train_recall.append((sum(epoch_recall) / len(epoch_recall)))\n",
    "            self.running_avg_train_accuracy.append((sum(epoch_accuracy) / len(epoch_accuracy)))\n",
    "            self.running_avg_train_loss.append((sum(epoch_loss) / len(epoch_loss)))\n",
    "\n",
    "            torch.save(self.running_avg_train_f1, os.path.join(self.object_dir, \"running_avg_train_f1.pt\"))\n",
    "            torch.save(self.running_avg_train_precision, os.path.join(self.object_dir, \"running_avg_train_precision.pt\"))\n",
    "            torch.save(self.running_avg_train_recall, os.path.join(self.object_dir, \"running_avg_train_recall.pt\"))\n",
    "            torch.save(self.running_avg_train_accuracy, os.path.join(self.object_dir, \"running_avg_train_accuracy.pt\"))\n",
    "            torch.save(self.running_avg_train_loss, os.path.join(self.object_dir, \"running_avg_train_loss.pt\"))\n",
    "\n",
    "            # tensorboard\n",
    "            if self.tensorboard_files:\n",
    "                self.train_summary_writer.add_scalar('train/F1', self.running_avg_train_f1[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('train/Precision', self.running_avg_train_precision[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('train/Recall', self.running_avg_train_recall[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('train/Accuracy', self.running_avg_train_accuracy[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('train/Loss', self.running_avg_train_loss[-1], global_step=self.globaliter)\n",
    "\n",
    "                self.train_summary_writer.add_scalar('F1', self.running_avg_train_f1[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('Precision', self.running_avg_train_precision[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('Recall', self.running_avg_train_recall[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('Accuracy', self.running_avg_train_accuracy[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('Loss', self.running_avg_train_loss[-1], global_step=self.globaliter)\n",
    "                                \n",
    "                self.train_summary_writer.flush()\n",
    "\n",
    "        except AttributeError:\n",
    "            self.running_avg_train_f1 = [(sum(epoch_f1) / len(epoch_f1))]\n",
    "            self.running_avg_train_precision = [(sum(epoch_precision) / len(epoch_precision))]\n",
    "            self.running_avg_train_recall = [(sum(epoch_recall) / len(epoch_recall))]\n",
    "            self.running_avg_train_accuracy = [(sum(epoch_accuracy) / len(epoch_accuracy))]\n",
    "            self.running_avg_train_loss = [(sum(epoch_loss) / len(epoch_loss))]\n",
    "\n",
    "        # print progress report\n",
    "        if self.verbose:\n",
    "            print(\"*\" * 10 + \"\\n\")\n",
    "            print(\"Train epoch: {} \\n\\tLoss: {:.6f} | F1: {:.6f} | Precision: {:.6f} | Recall: {:.6f} | Accuracy: {:.6f}\\n\".format(\n",
    "                        epoch, self.running_avg_train_loss[-1], self.running_avg_train_f1[-1],\n",
    "                        self.running_avg_train_precision[-1], self.running_avg_train_recall[-1], self.running_avg_train_accuracy[-1]))\n",
    "            print(\"\\tEpoch time elapsed: {}\".format(self.train_timer(epoch_beginning_time, epoch_ending_time)))\n",
    "            print(\"\\tTotal time elapsed: {}\".format(self.train_timer(self.beginning_time, time.time())))\n",
    "        \n",
    "        # capture globaliter\n",
    "        torch.save(self.globaliter, os.path.join(self.object_dir, \"globaliter.pt\"))\n",
    "\n",
    "    def validation(self, epoch):\n",
    "        epoch_preds = []\n",
    "        epoch_targets = []\n",
    "\n",
    "        epoch_f1 = []\n",
    "        epoch_precision = []\n",
    "        epoch_recall = []\n",
    "        epoch_accuracy = []\n",
    "        epoch_loss = []\n",
    "        \n",
    "        # sample batch number for data capture\n",
    "        num_batches = np.floor(len(self.validation_data_loader.dataset.image_paths) / self.validation_data_loader.batch_size)\n",
    "        sample_batch_idx = np.random.randint(0, num_batches)\n",
    "\n",
    "        # turn off gradients\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(self.validation_data_loader):\n",
    "                # reshape data as needed and send data to GPU if available\n",
    "                data = data.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "\n",
    "                # generate predictions\n",
    "                output = self.model(data)\n",
    "\n",
    "                validation_loss = self.criterion(output, target)\n",
    "                epoch_loss.append(validation_loss.item())\n",
    "\n",
    "                #Metrics\n",
    "                _, pred = torch.max(output, dim=1)\n",
    "                epoch_preds = epoch_preds + pred.detach().cpu().numpy().tolist()\n",
    "                epoch_targets = epoch_targets + target.detach().cpu().numpy().tolist()\n",
    "\n",
    "                metric_f1 = f1_score(epoch_targets, epoch_preds)\n",
    "                epoch_f1.append(metric_f1)\n",
    "\n",
    "                metric_precision = precision_score(epoch_targets, epoch_preds)\n",
    "                epoch_precision.append(metric_precision)\n",
    "\n",
    "                metric_recall = recall_score(epoch_targets, epoch_preds)\n",
    "                epoch_recall.append(metric_recall)\n",
    "\n",
    "                metric_accuracy = accuracy_score(epoch_targets, epoch_preds)\n",
    "                epoch_accuracy.append(metric_accuracy)\n",
    "            \n",
    "            # #\n",
    "            # if batch_idx == sample_batch_idx:\n",
    "            #     image_grid = torchvision.utils.make_grid(data.cpu())\n",
    "            #     self.validation_summary_writer.add_image('validation/Sample batch', image_grid, global_step=self.globaliter)\n",
    "                \n",
    "            # \n",
    "            try:\n",
    "                self.running_avg_validation_f1.append((sum(epoch_f1) / len(epoch_f1)))\n",
    "                self.running_avg_validation_precision.append((sum(epoch_precision) / len(epoch_precision)))\n",
    "                self.running_avg_validation_recall.append((sum(epoch_recall) / len(epoch_recall)))\n",
    "                self.running_avg_validation_accuracy.append((sum(epoch_accuracy) / len(epoch_accuracy)))\n",
    "                self.running_avg_validation_loss.append((sum(epoch_loss) / len(epoch_loss)))\n",
    "\n",
    "                torch.save(self.running_avg_validation_f1, os.path.join(self.object_dir, \"running_avg_validation_f1.pt\"))\n",
    "                torch.save(self.running_avg_validation_precision, os.path.join(self.object_dir, \"running_avg_validation_precision.pt\"))\n",
    "                torch.save(self.running_avg_validation_recall, os.path.join(self.object_dir, \"running_avg_validation_recall.pt\"))\n",
    "                torch.save(self.running_avg_validation_accuracy, os.path.join(self.object_dir, \"running_avg_validation_accuracy.pt\"))\n",
    "                torch.save(self.running_avg_validation_loss, os.path.join(self.object_dir, \"running_avg_validation_loss.pt\"))\n",
    "\n",
    "                # tensorboard\n",
    "                if self.tensorboard_files:\n",
    "                    # validation panel - one scalar per metric per plot\n",
    "                    self.validation_summary_writer.add_scalar('validation/F1', self.running_avg_validation_f1[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('validation/Precision', self.running_avg_validation_precision[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('validation/Recall', self.running_avg_validation_recall[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('validation/Accuracy', self.running_avg_validation_accuracy[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('validation/Loss', self.running_avg_validation_loss[-1], global_step=self.globaliter)\n",
    "\n",
    "                    # metric-specific plots\n",
    "                    self.validation_summary_writer.add_scalar('F1', self.running_avg_validation_f1[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('Precision', self.running_avg_validation_precision[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('Recall', self.running_avg_validation_recall[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('Accuracy', self.running_avg_validation_accuracy[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('Loss', self.running_avg_validation_loss[-1], global_step=self.globaliter)\n",
    "                    \n",
    "                    self.validation_summary_writer.flush()\n",
    "\n",
    "            # create statistics object and continue\n",
    "            except AttributeError:\n",
    "                self.running_avg_validation_f1 = [(sum(epoch_f1) / len(epoch_f1))]\n",
    "                self.running_avg_validation_precision = [(sum(epoch_precision) / len(epoch_precision))]\n",
    "                self.running_avg_validation_recall = [(sum(epoch_recall) / len(epoch_recall))]\n",
    "                self.running_avg_validation_accuracy = [(sum(epoch_accuracy) / len(epoch_accuracy))]\n",
    "                self.running_avg_validation_loss = [(sum(epoch_loss) / len(epoch_loss))]\n",
    "                \n",
    "            # print progress report\n",
    "            if self.verbose:\n",
    "                print(\"\\nValidation epoch: {} \\n\\tLoss: {:.6f} | F1: {:.6f} | Precision: {:.6f} | Recall: {:.6f} | Accuracy: {:.6f}\\n\".format(\n",
    "                        epoch, self.running_avg_validation_loss[-1], self.running_avg_validation_f1[-1],\n",
    "                        self.running_avg_validation_precision[-1], self.running_avg_validation_recall[-1], self.running_avg_validation_accuracy[-1]))\n",
    "        \n",
    "            # early stopping\n",
    "            if self.running_avg_validation_loss[-1] < self.min_val_loss:\n",
    "                # Save the model checkpoint\n",
    "                torch.save(self.model.state_dict(), os.path.join(self.model_dir, \"{}.pt\".format(self.model_name)))\n",
    "                self.epochs_no_improve = 0\n",
    "                self.min_val_loss = self.running_avg_validation_loss[-1]\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(\">>> Improved - saving model\\n\\n\\n\")\n",
    "\n",
    "            else:\n",
    "                self.epochs_no_improve += 1\n",
    "                if self.verbose:\n",
    "                    print(\">>> No improvement - {} consecutive epochs\\n\\n\\n\".format(self.epochs_no_improve))\n",
    "                if self.epochs_no_improve == self.n_epochs_stop:\n",
    "                    if self.verbose:\n",
    "                        print(\"\\n!!! Early stopping - {} epochs without improvement\\n\".format(self.n_epochs_stop))\n",
    "                self.running_avg_validation_loss = []\n",
    "                    \n",
    "    def train_timer(self, start, end):\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        return \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yj4ANAvfQm1B"
   },
   "source": [
    "## Parameter setup & execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ji_r8UDLQm8s"
   },
   "outputs": [],
   "source": [
    "# set input kwargs as object attributes\n",
    "class ParamConfig:  \n",
    "  def __init__(self, **kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "      setattr(self, key, value)\n",
    "\n",
    "# configure all necessary parameters\n",
    "model_params = ParamConfig(\n",
    "    model = model,\n",
    "    model_name = \"VGG16\",\n",
    "    model_object_dir = \"/content/drive/model_objects/20191202_1622_VGG16\",\n",
    "    # model_object_dir = None,\n",
    "    optimizer = torch.optim.Adam,\n",
    "    criterion = F.cross_entropy,\n",
    "    train_data_loader = train_data_loader,\n",
    "    validation_data_loader = validation_data_loader,\n",
    "    cuda = True if torch.cuda.is_available() else False,\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    seed = 0,\n",
    "    lr = 0.001,\n",
    "    epochs = 50,\n",
    "    tensorboard_files = True,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "### fit model\n",
    "# instantiate model object\n",
    "trainer = PyTorchTrainer(config=model_params)\n",
    "\n",
    "# iterate fitting procedure over specified epoch count\n",
    "for epoch in range(1, trainer.epochs + 1):\n",
    "    trainer.train(epoch)\n",
    "    trainer.validation(epoch)\n",
    "trainer.train_summary_writer.close()\n",
    "trainer.validation_summary_writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sexkKHd5AaI0"
   },
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFeslYBGAczq"
   },
   "outputs": [],
   "source": [
    "## wrong predictions\n",
    "# count of incorrect preds\n",
    "num_incorrect = len(target[pred != target])\n",
    "ix_incorrect = []\n",
    "if num_incorrect > 0:\n",
    "    \n",
    "    # capture indexes of incorrect predictions\n",
    "    for ix, (image, t, p) in enumerate(zip(data, target, preds)):\n",
    "        if t != p:\n",
    "            ix_incorrect.append(ix)\n",
    "            img_name = 'train/epoch-{}/batch-{}/prediction-{}/label-{}'.format(\n",
    "                epoch,\n",
    "                batch_idx,\n",
    "                labels_dict[p.cpu().item()],\n",
    "                labels_dict[t.cpu().item()]\n",
    "            )\n",
    "            self.train_summary_writer.add_image(img_name, image, global_step=str(epoch) + str(batch_idx) + str(ix))\n",
    "    \n",
    "    #     writer.add_image(img_name, image, epoch)\n",
    "    #     i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qNmjQwivAkP7"
   },
   "outputs": [],
   "source": [
    "## demo code\n",
    "model = model.to(\"cuda\")\n",
    "labels_dict = {0: 'MEL', 1: 'NONMEL'}\n",
    "\n",
    "break_num = 1\n",
    "for batch_idx, (data, target) in enumerate(train_data_loader):\n",
    "    print()\n",
    "    print(batch_idx)\n",
    "    \n",
    "    # reshape data as needed and send data to GPU if available\n",
    "    data = data.to(\"cuda\")\n",
    "    target = target.to(\"cuda\")\n",
    "    preds = model(data)\n",
    "    preds = torch.argmax(F.softmax(preds), dim=1)\n",
    "\n",
    "    # count of incorrect preds\n",
    "    num_incorrect = len(target[preds != target])\n",
    "\n",
    "    ix_incorrect = []\n",
    "    if num_incorrect > 0:\n",
    "        \n",
    "        # capture indexes of incorrect predictions\n",
    "        for ix, (image, t, p) in enumerate(zip(data, target, preds)):\n",
    "            if t != p:\n",
    "                ix_incorrect.append(ix)\n",
    "                img_name = 'Validation/Prediction-{}/Label-{}'.format(labels_dict[p.cpu().item()], labels_dict[t.cpu().item()])\n",
    "                print(img_name)\n",
    "                image_sample(image.cpu(), figsize = (3,3))\n",
    "                plt.show()\n",
    "        #     writer.add_image(img_name, image, epoch)\n",
    "        #     i += 1\n",
    "    print(target)\n",
    "    print(preds)\n",
    "    print(ix_incorrect)\n",
    "    \n",
    "    \n",
    "    if break_num == batch_idx:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWJBznHERktP"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDpoNMx-QnH4"
   },
   "source": [
    "## Reload model & objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IVRRF0fQDIlZ"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/model_objects/20191202_1622_VGG16\"\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckxzZM6TEb0k"
   },
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0SK_TJdKEd9i"
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "# freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# customize last step of model to the this task\n",
    "model.classifier[6] = nn.Sequential(\n",
    "    nn.Linear(4096, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(512, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Kz1YuipEiL-"
   },
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZ-teujGDAeh"
   },
   "outputs": [],
   "source": [
    "model_object_dir = \"/content/drive/model_objects/20191202_1622_VGG16\"\n",
    "\n",
    "# establish directory\n",
    "model_dir = os.path.join(model_object_dir, \"models\")\n",
    "object_dir = os.path.join(model_object_dir, \"objects\")\n",
    "log_dir = os.path.join(model_object_dir, \"logs\")\n",
    "log_train_dir = os.path.join(model_object_dir, \"logs\", \"train\")\n",
    "log_val_dir = os.path.join(model_object_dir, \"logs\", \"validation\")\n",
    "\n",
    "# load model\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, os.listdir(model_dir)[0])))\n",
    "model = model.to(\"cuda\")\n",
    "model_name = os.listdir(model_dir)[0].split(\".\")[0]\n",
    "\n",
    "# load statistics objects\n",
    "running_avg_train_f1 = torch.load(os.path.join(object_dir, \"running_avg_train_f1.pt\"))\n",
    "running_avg_train_precision = torch.load(os.path.join(object_dir, \"running_avg_train_precision.pt\"))\n",
    "running_avg_train_recall = torch.load(os.path.join(object_dir, \"running_avg_train_recall.pt\"))\n",
    "running_avg_train_accuracy = torch.load(os.path.join(object_dir, \"running_avg_train_accuracy.pt\"))\n",
    "running_avg_train_loss = torch.load(os.path.join(object_dir, \"running_avg_train_loss.pt\"))\n",
    "\n",
    "running_avg_validation_f1 = torch.load(os.path.join(object_dir, \"running_avg_validation_f1.pt\"))\n",
    "running_avg_validation_precision = torch.load(os.path.join(object_dir, \"running_avg_validation_precision.pt\"))\n",
    "running_avg_validation_recall = torch.load(os.path.join(object_dir, \"running_avg_validation_recall.pt\"))\n",
    "running_avg_validation_accuracy = torch.load(os.path.join(object_dir, \"running_avg_validation_accuracy.pt\"))\n",
    "running_avg_validation_loss = torch.load(os.path.join(object_dir, \"running_avg_validation_loss.pt\"))\n",
    "\n",
    "globaliter = torch.load(os.path.join(object_dir, \"globaliter.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kopN-pDGBoLi"
   },
   "outputs": [],
   "source": [
    "globaliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48UcEC1NFBJ7"
   },
   "outputs": [],
   "source": [
    "plt.plot(running_avg_train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n1rJnQYxFK4A"
   },
   "outputs": [],
   "source": [
    "plt.plot(running_avg_validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bc7UBVRABXtk"
   },
   "outputs": [],
   "source": [
    "plt.plot(running_avg_validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JOPUO8-5QWDd"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "PATH = \"models/mnist_hw1_q1_2.pt\"\n",
    "model = fcNet().to(device)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WryfsS2xWJZd"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJ3LE5Mq4WAl"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkajpblyTpek"
   },
   "source": [
    "## Visualizing Convolution Neural Networks using Pytorch\n",
    "\n",
    "https://towardsdatascience.com/visualizing-convolution-neural-networks-using-pytorch-3dfa8443e74e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7W-ITwARTplR"
   },
   "source": [
    "### Filter visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_JiInnlU8pI"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "alexnet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MdRAWJO0Tpr4"
   },
   "outputs": [],
   "source": [
    "def plot_weights(model, layer_num, single_channel = True, collated = False):\n",
    "    \"\"\"\n",
    "    model — Alexnet model or any trained model\n",
    "    layer_num — Convolution Layer number to visualize the weights\n",
    "    single_channel — Visualization mode\n",
    "    collated — Applicable for single-channel visualization only.\n",
    "    \"\"\"\n",
    "    #extracting the model features at the particular layer number\n",
    "    layer = model.features[layer_num]\n",
    "\n",
    "    #checking whether the layer is convolution layer or not \n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "    #getting the weight tensor data\n",
    "    weight_tensor = model.features[layer_num].weight.data\n",
    "\n",
    "    if single_channel:\n",
    "        if collated:\n",
    "        plot_filters_single_channel_big(weight_tensor)\n",
    "        else:\n",
    "        plot_filters_single_channel(weight_tensor)\n",
    "        \n",
    "    else:\n",
    "        if weight_tensor.shape[1] == 3:\n",
    "        plot_filters_multi_channel(weight_tensor)\n",
    "        else:\n",
    "        print(\"Can only plot weights with three channels with single channel = False\")\n",
    "        \n",
    "    else:\n",
    "    print(\"Can only visualize layers which are convolutional\")\n",
    "        \n",
    "#visualize weights for alexnet - first conv layer\n",
    "plot_weights(alexnet, 0, single_channel = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W2eqJSwzTpyl"
   },
   "outputs": [],
   "source": [
    "#getting the weight tensor data\n",
    "weight_tensor = model.features[layer_num].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x1lAegMTTp4D"
   },
   "outputs": [],
   "source": [
    "#visualize weights for alexnet — first conv layer\n",
    "plot_weights(alexnet, 0, single_channel = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bObA8EfTp-i"
   },
   "source": [
    "### Image Occlusion Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPAT0rj7VXZa"
   },
   "outputs": [],
   "source": [
    "#for visualization we will use vgg16 pretrained on imagenet data\n",
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yp01LVvnVXf0"
   },
   "outputs": [],
   "source": [
    "def occlusion(model, image, label, occ_size = 50, occ_stride = 50, occ_pixel = 0.5):\n",
    "  \n",
    "    #get the width and height of the image\n",
    "    width, height = image.shape[-2], image.shape[-1]\n",
    "  \n",
    "    #setting the output image width and height\n",
    "    output_height = int(np.ceil((height-occ_size)/occ_stride))\n",
    "    output_width = int(np.ceil((width-occ_size)/occ_stride))\n",
    "  \n",
    "    #create a white image of sizes we defined\n",
    "    heatmap = torch.zeros((output_height, output_width))\n",
    "    \n",
    "    #iterate all the pixels in each column\n",
    "    for h in range(0, height):\n",
    "        for w in range(0, width):\n",
    "            \n",
    "            h_start = h*occ_stride\n",
    "            w_start = w*occ_stride\n",
    "            h_end = min(height, h_start + occ_size)\n",
    "            w_end = min(width, w_start + occ_size)\n",
    "            \n",
    "            if (w_end) >= width or (h_end) >= height:\n",
    "                continue\n",
    "            \n",
    "            input_image = image.clone().detach()\n",
    "            \n",
    "            #replacing all the pixel information in the image with occ_pixel(grey) in the specified location\n",
    "            input_image[:, :, w_start:w_end, h_start:h_end] = occ_pixel\n",
    "            \n",
    "            #run inference on modified image\n",
    "            output = model(input_image)\n",
    "            output = nn.functional.softmax(output, dim=1)\n",
    "            prob = output.tolist()[0][label]\n",
    "            \n",
    "            #setting the heatmap location to probability value\n",
    "            heatmap[h, w] = prob \n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yuf9GnV3VXl9"
   },
   "outputs": [],
   "source": [
    "\n",
    "#compute occlusion heatmap\n",
    "heatmap = occlusion(model, images, pred[0].item(), 32, 14)\n",
    "\n",
    "#displaying the image using seaborn heatmap and also setting the maximum value of gradient to probability\n",
    "imgplot = sns.heatmap(heatmap, xticklabels=False, yticklabels=False, vmax=prob_no_occ)\n",
    "figure = imgplot.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hqnHwUoRVXr_"
   },
   "source": [
    "## How to train an image classifier\n",
    "\n",
    "https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dOjowz9aVXxS"
   },
   "outputs": [],
   "source": [
    "def predict_image(image):\n",
    "    image_tensor = test_transforms(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input = Variable(image_tensor)\n",
    "    input = input.to(device)\n",
    "    output = model(input)\n",
    "    index = output.data.cpu().numpy().argmax()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_HLDraEKVX5U"
   },
   "outputs": [],
   "source": [
    "def get_random_images(num):\n",
    "    data = datasets.ImageFolder(data_dir, transform=test_transforms)\n",
    "    classes = data.classes\n",
    "    indices = list(range(len(data)))\n",
    "    np.random.shuffle(indices)\n",
    "    idx = indices[:num]\n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "    sampler = SubsetRandomSampler(idx)\n",
    "    loader = torch.utils.data.DataLoader(data, \n",
    "                   sampler=sampler, batch_size=num)\n",
    "    dataiter = iter(loader)\n",
    "    images, labels = dataiter.next()\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhFpO-kATqEH"
   },
   "outputs": [],
   "source": [
    "to_pil = transforms.ToPILImage()\n",
    "images, labels = get_random_images(5)\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "for ii in range(len(images)):\n",
    "    image = to_pil(images[ii])\n",
    "    index = predict_image(image)\n",
    "    sub = fig.add_subplot(1, len(images), ii+1)\n",
    "    res = int(labels[ii]) == index\n",
    "    sub.set_title(str(classes[index]) + \":\" + str(res))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JFUBWEkXK8J"
   },
   "source": [
    "## Imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4ib4zbVXLCh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZo9TXbHXLH4"
   },
   "source": [
    "## Captum insights\n",
    "\n",
    "https://towardsdatascience.com/facebook-has-been-quietly-open-sourcing-some-amazing-deep-learning-capabilities-for-pytorch-a7ed5bc71f26\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SttyDbQkXLNW"
   },
   "source": [
    "## Extract edge features\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u09nq_bhXLSy"
   },
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import numpy as np\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.filters import prewitt_h,prewitt_v\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#reading the image \n",
    "image = imread('puppy.jpeg',as_gray=True)\n",
    "\n",
    "#calculating horizontal edges using prewitt kernel\n",
    "edges_prewitt_horizontal = prewitt_h(image)\n",
    "#calculating vertical edges using prewitt kernel\n",
    "edges_prewitt_vertical = prewitt_v(image)\n",
    "\n",
    "imshow(edges_prewitt_vertical, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzOsu_9jbIYR"
   },
   "source": [
    "## Interpreting deep learning models\n",
    "\n",
    "https://medium.com/google-developer-experts/interpreting-deep-learning-models-for-computer-vision-f95683e23c1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "op6ZiPw4bIhL"
   },
   "source": [
    "### SHAP gradient explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mkIwEr_KbIo6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import keras.backend as K\n",
    "import json\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "\n",
    "# utility function to visualize SHAP values in larger image formats\n",
    "# this modifies the `shap.image_plot(...)` function\n",
    "def visualize_model_decisions(shap_values, x, labels=None, figsize=(20, 30)):\n",
    "    \n",
    "    colors = []\n",
    "    for l in np.linspace(1, 0, 100):\n",
    "        colors.append((30./255, 136./255, 229./255,l))\n",
    "    for l in np.linspace(0, 1, 100):\n",
    "        colors.append((255./255, 13./255, 87./255,l))\n",
    "    red_transparent_blue = LinearSegmentedColormap.from_list(\"red_transparent_blue\", colors)\n",
    "\n",
    "    multi_output = True\n",
    "    if type(shap_values) != list:\n",
    "        multi_output = False\n",
    "        shap_values = [shap_values]\n",
    "\n",
    "    # make sure labels\n",
    "    if labels is not None:\n",
    "        assert labels.shape[0] == shap_values[0].shape[0], \"Labels must have same row count as shap_values arrays!\"\n",
    "        if multi_output:\n",
    "            assert labels.shape[1] == len(shap_values), \"Labels must have a column for each output in shap_values!\"\n",
    "        else:\n",
    "            assert len(labels.shape) == 1, \"Labels must be a vector for single output shap_values.\"\n",
    "\n",
    "    # plot our explanations\n",
    "    fig_size = figsize\n",
    "    fig, axes = plt.subplots(nrows=x.shape[0], ncols=len(shap_values) + 1, figsize=fig_size)\n",
    "    if len(axes.shape) == 1:\n",
    "        axes = axes.reshape(1,axes.size)\n",
    "    for row in range(x.shape[0]):\n",
    "        x_curr = x[row].copy()\n",
    "\n",
    "        # make sure\n",
    "        if len(x_curr.shape) == 3 and x_curr.shape[2] == 1:\n",
    "            x_curr = x_curr.reshape(x_curr.shape[:2])\n",
    "        if x_curr.max() > 1:\n",
    "            x_curr /= 255.\n",
    "        \n",
    "        axes[row,0].imshow(x_curr)\n",
    "        axes[row,0].axis('off')\n",
    "        \n",
    "        # get a grayscale version of the image\n",
    "        if len(x_curr.shape) == 3 and x_curr.shape[2] == 3:\n",
    "            x_curr_gray = (0.2989 * x_curr[:,:,0] + 0.5870 * x_curr[:,:,1] + 0.1140 * x_curr[:,:,2]) # rgb to gray\n",
    "        else:\n",
    "            x_curr_gray = x_curr\n",
    "\n",
    "        if len(shap_values[0][row].shape) == 2:\n",
    "            abs_vals = np.stack([np.abs(shap_values[i]) for i in range(len(shap_values))], 0).flatten()\n",
    "        else:\n",
    "            abs_vals = np.stack([np.abs(shap_values[i].sum(-1)) for i in range(len(shap_values))], 0).flatten()\n",
    "        max_val = np.nanpercentile(abs_vals, 99.9)\n",
    "        for i in range(len(shap_values)):\n",
    "            if labels is not None:\n",
    "                axes[row,i+1].set_title(labels[row,i])\n",
    "            sv = shap_values[i][row] if len(shap_values[i][row].shape) == 2 else shap_values[i][row].sum(-1)\n",
    "            axes[row,i+1].imshow(x_curr_gray, cmap=plt.get_cmap('gray'), alpha=0.15, extent=(-1, sv.shape[0], sv.shape[1], -1))\n",
    "            im = axes[row,i+1].imshow(sv, cmap=red_transparent_blue, vmin=-max_val, vmax=max_val)\n",
    "            axes[row,i+1].axis('off')\n",
    "        \n",
    "    cb = fig.colorbar(im, ax=np.ravel(axes).tolist(), label=\"SHAP value\", orientation=\"horizontal\", aspect=fig_size[0]/0.2)\n",
    "    cb.outline.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F0mEmv3LbIvI"
   },
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ztdj9epVbI27"
   },
   "outputs": [],
   "source": [
    "# get imagenet id to label name mappings\n",
    "fname = shap.datasets.cache(url)\n",
    "with open(fname) as f:\n",
    "    class_names = json.load(f)\n",
    "    \n",
    "# make model predictions\n",
    "predictions = model.predict(preprocess_input(to_predict.copy()))\n",
    "\n",
    "# get prediction labels\n",
    "predicted_labels = [class_names.get(str(pred)) for pred in np.argmax(predictions, axis=1)]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yz0EOnmKcQoI"
   },
   "outputs": [],
   "source": [
    "# utility function to pass inputs to specific model layers\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "    \n",
    "# focus on the 7th layer of CNN model\n",
    "print(model.layers[7].input)\n",
    "Out [46]: <tf.Tensor 'block2_pool_2/MaxPool:0' shape=(?, 56, 56, 128) dtype=float32>\n",
    "\n",
    "# make model predictions\n",
    "e = shap.GradientExplainer((model.layers[7].input, model.layers[-1].output), \n",
    "                            map2layer(preprocess_input(X.copy()), 7))\n",
    "shap_values, indexes = e.shap_values(map2layer(to_predict, 7), ranked_outputs=2)\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "print(index_names)\n",
    "Out [47]: array([['chain', 'chain_mail'],\n",
    "                 ['great_grey_owl', 'prairie_chicken'],\n",
    "                 ['desktop_computer', 'screen'],\n",
    "                 ['Egyptian_cat', 'tabby']], dtype='<U16')\n",
    "\n",
    "# visualize model decisions\n",
    "visualize_model_decisions(shap_values=shap_values, x=to_predict, \n",
    "                          labels=index_names, figsize=(20, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJnTE1ypcQuJ"
   },
   "outputs": [],
   "source": [
    "# focus on 14th layer of the CNN model\n",
    "print(model.layers[14].input)\n",
    "Out [49]: <tf.Tensor 'block4_conv3_2/Relu:0' shape=(?, 28, 28, 512) dtype=float32>\n",
    "\n",
    "# make model predictions\n",
    "e = shap.GradientExplainer((model.layers[14].input, model.layers[-1].output), \n",
    "                            map2layer(preprocess_input(X.copy()), 14))\n",
    "shap_values, indexes = e.shap_values(map2layer(to_predict, 14), ranked_outputs=2)\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# visualize model decisions\n",
    "visualize_model_decisions(shap_values=shap_values, x=to_predict, \n",
    "                          labels=index_names, figsize=(20, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NxWSx281cQ0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iu6NU7iNcQ6e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zRAXXtZXLYn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CjnaX3Z_DEER"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hg2iQ0Wt-XGh",
    "sexkKHd5AaI0",
    "dJ3LE5Mq4WAl"
   ],
   "machine_shape": "hm",
   "name": "ProjectWorkflow_TP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
